{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> FUNDAMENTOS DE APRENDIZAJE AUTOMÁTICO <br> Y RECONOCIMIENTO DE PATRONES</center>\n",
    "## <center> 2do parcial, 2021</center>           \n",
    "\n",
    "La duración del parcial es de 3 horas. El parcial consta de 3 ejercicios, cuya suma total es de 100 puntos. El parcial es sin material y no está permitido acceder a Internet. Ante cualquier duda comuníquese con los docentes. \n",
    "\n",
    "Este notebook corresponde al ejercicio 1. Hay un notebook por ejercicio planteado.\n",
    "\n",
    "* [Ejercicio 1 - Redes Neuronales](#Ejercicio1) (35 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan las bibliotecas que se utilizarán\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fuaa_utils_ej1 import validar_resultado\n",
    "from fuaa_utils_ej1 import error_relativo\n",
    "from fuaa_utils_ej1 import calcular_gradiente_numerico_array\n",
    "from fuaa_utils_ej1 import calcular_gradiente_numerico\n",
    "from fuaa_utils_ej1 import mostrar_ajuste\n",
    "from fuaa_utils_ej1 import identificar_parcial\n",
    "\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "identificar_parcial()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Ejercicio1\"></a>\n",
    "## Ejercicio 1: Red neuronal de tres capas para predecir calidad del vino\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción del ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio se implementarán algunos de los bloques constitutivos de una red neuronal de **tres capas**. La siguiente figura muestra el diagrama de bloques de la red que se implementará. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/red_tres_capas.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se describen los bloques:\n",
    "- **Inicializar parámetros:** Inicializa los parámentros de la red. A los pesos de la capa $l$ de la red les llamaremos $W_l$, $b_l$ con $l=1,2,3$. \n",
    "- **Propagación hacia adelante:** La *propagación hacia adelante*  o *forward propagation* consiste en estimar la salida de la red a partir de la entrada. Cada nodo o capa de la red tiene un método *forward* asociado. Se proveen las implementaciones de los métodos forward asociados a los siguientes bloques:\n",
    "        - Afin\n",
    "        - Activación \n",
    "        - Afin --> Activación\n",
    "        \n",
    "- **Loss:** Calcula el valor de la función de costo a optimizar. Se implementará como función de costo:\n",
    "        - Mean Square Error\n",
    "- **Propagación hacia atrás:** Durante la *propagación hacia atrás* o *backpropagation* se calculan los gradientes necesarios para actualizar los parámetros de la red. Se implementarán métodos *backward* para los siguientes bloques:\n",
    "        - Afin\n",
    "        - Activación \n",
    "        - Afin --> Activación\n",
    "- **Update:** Es el boque encargado de actualizar los parámetros. Para ello utiliza los gradientes calculados durante la *propagación hacia atrás* y un método de optimización. Se utilizará *descenso por gradiente* como método de optimización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Bloque de Inicialización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementará el bloque de inicialización para el caso de una red neuronal de **tres capas** con la siguiente estructura:    \n",
    "  Afin --> Activación 1 --> Afin --> Activacion 2 --> Afin --> Activación 3       \n",
    "\n",
    "### Parte a) \n",
    "Completar la implementación de `inicializar_pesos()`. Los pesos $W_l$ serán inicializados en valores aleatorios con distribución gaussiana de desviación estandar $\\sigma_l=1/\\sqrt{d_{l-1}}$, siendo $d_{l-1}$ el número de nodos de la capa $l-1$. Por ejemplo, para la primera capa $l=1$, la cantidad de nodos $d_{l-1}=d_0$ corresponde a la dimensión del vector de características. Los pesos correspondientes a términos de *bias* se inicializarán a cero. \n",
    "\n",
    "**Nota:** No necesario realizar una implementación genérica. Alcanza con que funcione para una red de tres capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializar_pesos(dims, semilla=1):\n",
    "    \"\"\"\n",
    "    Entrada:\n",
    "        dims: lista que contiene el número de nodos de cada una de las capas. El primer elemento\n",
    "              corresponde al tamaño del vector de características y el último a la cantidad de \n",
    "              nodos en la última capa oculta.\n",
    "        semilla: semilla a utilizar para generar los valores aleatorios\n",
    "    \n",
    "    Salida:\n",
    "        parametros: diccionario de python que contiene los parámetros inicializados \n",
    "                    parametros['W' + str(l)] = ... \n",
    "                    parametros['b' + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(semilla)\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    # Sugerencia: puede ser útil utilizar np.random.randn() y ajustar la desviación estándar\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "   \n",
    "    # Se genera el diccionario con los valores inicializados\n",
    "    parametros = {'W1': W1,\n",
    "                  'b1': b1,\n",
    "                  'W2': W2,\n",
    "                  'b2': b2,\n",
    "                  'W3': W3,\n",
    "                  'b3': b3}\n",
    "    \n",
    "    return parametros    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se testea la inicialización con pesos aleatorios\n",
    "validar_resultado(\"inicializar_pesos\", funcion=inicializar_pesos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Bloques Forward\n",
    "\n",
    "Se proveen las implementaciones de los métodos *forward* de los siguientes bloques: \n",
    "\n",
    "- Bloque Afin  \n",
    "- Bloque Activación donde la activación puede ser ReLU, Sigmoide\n",
    "- Bloque Afin -> Activación  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Forward Afin\n",
    "\n",
    "La señal de entrada a la activación de la capa $\\textit{l}$ puede escribirse como:\n",
    "\n",
    "$$\n",
    "\\mathbf{s}^{(l)}=\\left( W^{(l)} \\right)^T \\mathbf{x}^{(l-1)}+ \\mathbf{b}^{(l)}   \\tag{1}\n",
    "$$\n",
    "\n",
    "donde $\\mathbf{s}^{(l)}$ y $\\mathbf{b}^{(l)}$ son vectores de tamaño $d^{(l)}$, $\\mathbf{x}^{(l-1)}$  es un vector de tamaño $d^{(l-1)}$ y $W^{(l)}$ es una matriz de tamaño $d^{(l-1)} \\times d^{(l)}$.\n",
    "\n",
    "La ecuación (1) es válida cuando la entrada a la capa es un único vector $\\mathbf{x}^{(l-1)}$. En la práctica es más habitual procesar un $\\textit{batch}$ de vectores de entrada a la vez, por lo tanto es deseable contar con una expresión que genere la salida para todos los vectores de entrada a la vez. Al evitar la utilización de un bloque $\\textit{for}$ que itere por cada una de las muestras del $\\textit{batch}$ se mejora la eficiencia de la implementación.   \n",
    "\n",
    "\n",
    "La versión de la ecuación (1) que actúa sobre un conjunto de muestras a la vez es la siguiente:\n",
    "\n",
    "$$\n",
    "S^{(l)} = X^{(l-1)}W^{(l)} +b^{(l)}\\tag{2}\n",
    "$$\n",
    "\n",
    "donde $X^{[0]} = X$, siendo X una matriz que contiene un vector de características en cada fila."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afin_forward(X, W, b):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia adelante en una capa afin.\n",
    "\n",
    "    Entrada:\n",
    "        X: matriz de tamaño (N, dim capa anterior) que en cada fila contiene un vector de\n",
    "           activaciones de la capa anterior (o datos de entrada)\n",
    "        W: matriz de pesos de tamaño (dim de capa anterior, dim de capa actual) \n",
    "        b: vector de bias de tamaño (dim de la capa actual,)\n",
    "\n",
    "    Salida:\n",
    "        S: matriz de tamaño (N, dim de capa actual) que contiene\n",
    "           los scores o señal de entrada a la activación  \n",
    "        cache: (X, W, b) tupla que contiene X, W y b. \n",
    "               Son almacenados para calcular el paso backward eficientemente\n",
    "    \"\"\"\n",
    "\n",
    "    S = np.dot(X, W) + b\n",
    "    \n",
    "    assert(S.shape == (X.shape[0], W.shape[1] ))\n",
    "    cache = (X, W, b)\n",
    "    \n",
    "    return S, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Funciones de activación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se proveen las implementaciones de las siguientes funciones de activación:\n",
    "\n",
    "- **Sigmoide**: $\\sigma(S) = \\sigma(X W  + b) = \\frac{1}{ 1 + e^{-(X W  + b)}}$. Esta función devuelve, además de la activación resultante, la variable cache que contiene la señal `S` que dio lugar a la activación (se utiilza luego durante la propagación hacia atrás).\n",
    "\n",
    "``` python\n",
    "X, cache = sigmoid(S)\n",
    "```\n",
    "\n",
    "\n",
    "- **Rectified Linear Unit**:  $ReLU(S) = \\max(0, S)$.  Al igual que en el caso de la activación sigmoide, esta función devuelve además de la activación resultante, la variable cache que contiene la señal `S` que dio lugar a la activación (se utiilza luego durante la propagación hacia atrás).\n",
    "\n",
    "``` python\n",
    "X, cache = relu(S)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(S):\n",
    "    \"\"\"\n",
    "    Implementa la activación sigmoide\n",
    "    \n",
    "    Entrada:\n",
    "        S: arreglo numpy que contiene las entradas a la activación. \n",
    "           Las dimensiones de entrada no están definidas.\n",
    "    \n",
    "    Salida:\n",
    "        X: arreglo del mismo tamaño que S que contiene la salida de sigmoide(S) \n",
    "    cache: devuelve S para utilizar durante la propagación hacia atrás\n",
    "    \"\"\"\n",
    "    \n",
    "    X = 1/(1+np.exp(-S))\n",
    "    cache = S\n",
    "\n",
    "    assert X.shape == S.shape, 'La entrada y la salida deben ser del mismo tamaño'\n",
    "    return X, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(S):\n",
    "    '''\n",
    "    Implementa la activación relu\n",
    "    \n",
    "    Entrada:\n",
    "        S: arreglo numpy que contiene las entradas a la activación. \n",
    "           Las dimensiones de entrada no están definidas.\n",
    "    \n",
    "    Salida:\n",
    "        X: arreglo del mismo tamaño que S que contiene la salida de relu(S) \n",
    "    cache: devuelve S para utilizar durante la propagación hacia atrás\n",
    "    '''\n",
    "    \n",
    "    X = np.maximum(0,S)\n",
    "    \n",
    "    assert(X.shape == S.shape)\n",
    "    cache = S \n",
    "        \n",
    "    return X, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Aplicación conjunta de capa afin y activación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se provee la implementación de la propagación hacia adelante de una capa *Afin->Activacion*. El método `afin_activacion_forward()` implementa la operación:\n",
    "\n",
    "$$\n",
    "X^{[l]} = \\theta(S^{(l)}) = \\theta(X^{(l-1)}W^{(l)} +b^{(l)})   \\tag{3}\n",
    "$$\n",
    "\n",
    "donde la activación $\\theta(\\cdot)$ será alguna de las implementadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afin_activacion_forward(X_prev, W, b, activacion):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia adelante para una capa Afin->Activación \n",
    "    Entrada:\n",
    "        X_prev: arreglo de tamaño (N, dim capa anterior) que contiene la \n",
    "                activación de la capa anterior (o datos de entrada):          \n",
    "        W: matriz de pesos de tamaño (dim de capa anterior, dim de capa actual)  \n",
    "        b: vector de bias de tamaño (dim de la capa actual)\n",
    "        activacion: la activacion a utilizar en esta capa se indica con uno de los \n",
    "                    siguientes strings: 'sigmoide' o 'relu'\n",
    "\n",
    "    Salida:\n",
    "        X: arreglo de tamaño (N, dim de capa actual) que contiene la salida \n",
    "           de la función de activación  \n",
    "    cache: tupla que contiene \"cache_afin\" y \"cache_activacion\".\n",
    "           Se almacenan para calcular la propagación hacia atrás eficientemente\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    S, cache_afin = afin_forward(X_prev,W,b)\n",
    "    \n",
    "    if activacion == 'sigmoide':    \n",
    "        \n",
    "        X, cache_activacion = sigmoide(S)\n",
    "    \n",
    "    elif activacion == 'relu':\n",
    "        \n",
    "        X, cache_activacion = relu(S)\n",
    "    \n",
    "    assert (X.shape == (X_prev.shape[0], W.shape[1]))\n",
    "    cache = (cache_afin, cache_activacion)\n",
    "\n",
    "    return X, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Función de costo\n",
    "\n",
    "Se implementará la siguiente función de costo:  \n",
    "\n",
    "- **Error cuadrático medio:** Es la función de costo más utilizada en problemas de regresión. Se recuerda que la misma se define mediante la fórmula:\n",
    "$$\n",
    "MSE(\\mathbf{x}^{(L)}, \\mathbf{y})= \\frac{1}{2N} \\sum\\limits_{n = 1}^{N} \\left(y_n - x^{(L)}_n \\right)^2 \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte b) \n",
    "Completar la implementación del método `mse()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(xL, y):\n",
    "    \"\"\"\n",
    "    Implementa el error cuadratico medio como función de costo de una red neuronal con una sola capa\n",
    "    de salida.\n",
    "\n",
    "    Entrada:\n",
    "        xL: vector de dimensión (N,1) que contiene las salidas generadas por la red neuronal para N muestras.\n",
    "        y:  vector de dimensión (N,1) que contiene las salidas esperadas\n",
    "\n",
    "    Salida:\n",
    "        costo: escalar con el costo calculado\n",
    "        dxL: gradiente del costo respecto a xL, tiene las mismas dimensiones que xL\n",
    "    \"\"\"\n",
    "    \n",
    "    N = len(y)\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "\n",
    "    costo = np.squeeze(costo) # Para asegurarnos que la salida sea un escalar (Ej: transforma [[12]] en 12).\n",
    "    assert(costo.shape == ())\n",
    "    assert(dxL.shape == xL.shape), 'Las dimensiones de dxL y xL deben ser iguales'\n",
    "    return costo, dxL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validar_resultado('mse', funcion=mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Propagación hacia atrás"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se implementará la versión *backward* de cada una de las funciones *forward* implementadas anteriormente. A saber:\n",
    "- AFIN backward\n",
    "- ACTIVACION backward \n",
    "- AFIN -> ACTIVACION backward donde ACTIVACION puede ser *ReLU* o *sigmoide* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Afin backward\n",
    "\n",
    "Durante la propagación hacia adelante en la capa $l$ (sin considerar la activación) se calcula para una muestra: \n",
    "\n",
    "$$\n",
    "\\mathbf{s}^{(l)}=\\left( W^{(l)} \\right)^T \\mathbf{x}^{(l-1)}+ \\mathbf{b}^{(l)}   \\tag{1}\n",
    "$$\n",
    "\n",
    "Si se llama $e_n$ al costo debido a la muesta $n$ y se asume conocido el *vector de sensibilidad* $\\delta^{(l)}=\\frac{\\partial e_n}{\\partial \\mathbf{s}^{(l)}}$, en el teórico del curso se vio que \n",
    "\n",
    "$$\n",
    "\\frac{\\partial{e_n}}{\\partial{W^{(l)}}}=\\mathbf{x}^{(l-1)} \\left( \\delta^{(l)} \\right)^T\n",
    "$$\n",
    "\n",
    "Análogamente a como se hizo en el caso de la propagación hacia adelante, si se considera la contribución al error de un conjunto de muestras a la vez la ecuación se puede escribir en forma vectorizada como:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{W^{(l)}}}= dW^{(l)} = \\left( X^{(l-1)}\\right)^ T dS^{(l)}   \\tag{5}\n",
    "$$\n",
    "\n",
    "donde $dS^{(l)}$ es una matríz de tamaño $N\\times d^{(l)}$ que en cada fila contiene el vector de sensibilidad $\\delta^{(l)}_n$ correspondiente a una de las muestras.\n",
    "\n",
    "Las derivadas respecto al vector de bias $\\mathbf{b}^{(l)}$ se calculan de forma similar (puede pensarse como un caso particular en que $X^{(l-1)}$ es un vector columna de unos) por lo que\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{E}}{\\partial{\\mathbf{b}^{(l)}}}= d\\mathbf{b}^{(l)} =\\mathbb{1} ^ T dS^{(l)}  \\tag{6}\n",
    "$$\n",
    "\n",
    "Finalmente se calcula la influencia de cada una de las características en el error. Considerando primero el caso de una muestra, se tiene que:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{e_n}}{\\partial{\\mathbf{x}^{(l-1)}}} = W^{(l)} \\delta^{(l)}\n",
    "$$\n",
    "\n",
    "que en forma vectorizada puede escribirse como:\n",
    "\n",
    "$$ \n",
    " \\frac{\\partial E }{\\partial X^{(l-1)}} = dX^{(l-1)} = dS^{(l)} \\left( W^{(l) }\\right)^T \\tag{7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte c) \n",
    "Implementar el método `afin_backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afin_backward(dS, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás para una capa l (sin considerar la activación)\n",
    "\n",
    "    Entrada:\n",
    "        dS: Gradiente de la función de costo con respecto a la salida de la capa actual \n",
    "            (sin considerar la activación)\n",
    "        cache: tupla de valores (X_prev, W, b) calculados durante la propagación hacia adelante\n",
    "               de la capa actual\n",
    "\n",
    "    Salida:\n",
    "        dX_prev: Gradiente de la función de costo con respecto a la activación de la capa anterior (l-1), \n",
    "                 tiene el mismo tamaño que X_prev\n",
    "        dW: Gradiente de la función de costo con respecto a W (de la capa actual l), \n",
    "            tiene el mismo tamaño que W\n",
    "        db: Gradiente de la función de costo con respecto a b (de la capa actual l), \n",
    "            tiene el mismo tamaño que b\n",
    "    \"\"\"\n",
    "    X_prev, W, b = cache\n",
    "    N = X_prev.shape[0]\n",
    "\n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "\n",
    "    assert (dX_prev.shape == X_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dX_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validar_resultado('afin_backward', funcion=afin_backward, f_forward=afin_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Activación backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si  $\\theta(\\cdot)$ es la función de activación, entonces su función *backward* se calcula \n",
    "\n",
    "$$\n",
    "dS^{(l)} = dX^{(l)} * \\theta'(S^{(l)})   \\tag{8}\n",
    "$$.  \n",
    "\n",
    "donde $\\theta'(\\cdot)$ debe ser calculado para cada caso. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte d) \n",
    "Implementar los métodos *backward* cada una de las funciones de activación implementadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide_backward(dX, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás de una activación Sigmoide.\n",
    "\n",
    "    Entrada:\n",
    "        dX: gradiente de la función de costo respecto a la salida de la capa relu,\n",
    "              el tamaño del arreglo no está definido\n",
    "        cache: 'S' valor almacenado durante la propagación hacia adelante\n",
    "\n",
    "    Salida:\n",
    "        dS: gradiente del costo respecto a S\n",
    "    \"\"\"\n",
    "    \n",
    "    S = cache\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "     \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "    assert (dS.shape == S.shape), 'dS y S no tienen el mismo tamaño'\n",
    "    assert (dX.shape == S.shape), 'dX y S no tienen el mismo tamaño'\n",
    "    \n",
    "    return dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validar_resultado('activacion_backward', f_backward=sigmoide_backward, f_forward=sigmoide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dX, cache):\n",
    "    \"\"\"\n",
    "    Implementa la propagación hacia atrás de una activación ReLu.\n",
    "\n",
    "    Entrada:\n",
    "        dX: gradiente de la función de costo respecto a la salida de la capa relu,\n",
    "              el tamaño del arreglo no está definido\n",
    "        cache: 'S' valor almacenado durante la propagación hacia adelante\n",
    "\n",
    "    Salida:\n",
    "        dS: gradiene del costo respecto a S\n",
    "    \"\"\"\n",
    "    \n",
    "    S = cache\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "    assert (dS.shape == S.shape)\n",
    "    \n",
    "    return dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validar_resultado('activacion_backward', f_backward=relu_backward, f_forward=relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Afin --> Activacion backward\n",
    "\n",
    "A continuación se implementará la función que realiza la propagación hacia atrás del la capa *Afin-->Activacion*. \n",
    "\n",
    "### Parte e) \n",
    "Implementar la función `afin_activacion_backward()`. Para ello utilizar las funciones implementadas anteriormente: `afin_backward` y la `activacion_backward` que corresponda. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afin_activacion_backward(dX, cache, activacion):\n",
    "    '''\n",
    "    Implementar la propagación hacia atrás para la capa Afin->Activacion.\n",
    "    \n",
    "    Entradas:\n",
    "        dX: gradiente del costo respecto a la salida de la capa actual \n",
    "        cache: tupla con los valores(cache_afin, cache_activacion) \n",
    "        activacion: la activación a utilizar en esta capa, puede ser 'sigmoide' o 'relu'\n",
    "    Salidas:\n",
    "        dX_prev: Gradiente del costo con respecto a la activación de la capa anterior(l-1), \n",
    "                 tiene las mismas dimensiones que X_prev\n",
    "        dW -- Gradiente del costo con respecto a W (de la capa actual l), \n",
    "              tiene las mismas dimensiones que W\n",
    "        db -- Gradiente del costo con respecto a b (de la capa actual l), \n",
    "              tiene las mismas dimensiones que b\n",
    "    '''\n",
    "    cache_afin, cache_activacion = cache\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "    return dX_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validar_resultado('afin_activacion_backward', f_backward=afin_activacion_backward, f_forward=afin_activacion_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  5 - Actualización de los parámetros\n",
    "\n",
    "En esta sección se actualizarán los parámetros del modelo mediante el método de *descenso por gradiente*:\n",
    "\n",
    "$$ W^{(l)} = W^{(l)} -\\eta \\text{ } dW^{(l)} \\tag{9}$$\n",
    "$$ \\mathbf{b}^{(l)} = \\mathbf{b}^{(l)} -\\eta \\text{ } \\mathbf{db}^{(l)} \\tag{10}$$\n",
    "\n",
    "donde $\\eta$ es el *learning rate*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte f) \n",
    "Implementar `actualizar_parametros()` para actualizar los parámetros usando *descenso por gradiente*. Luego de actualizar los parámetros, almacenarlos en el diccionario de parámetros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actualizar_parametros(parametros, gradientes, learning_rate):\n",
    "    \"\"\"\n",
    "    Se actualizan los parámetros utilizando descenso por gradiente. Si bien en este notebook se trabaja \n",
    "    con una red de tres capas, el método se implementa en forma genérica para mostrar como se haría en el\n",
    "    caso más general.\n",
    "    \n",
    "    Entrada:\n",
    "        parametros: diccionario de python que contiene los parámetros \n",
    "                        parametros[\"W\" + str(l)] = ... \n",
    "                        parametros[\"b\" + str(l)] = ...\n",
    "        gradientes: diccionario de python que contiene los gradientes \n",
    "                    (las salidas de los métodos backward)\n",
    "                        gradientes[\"W\" + str(l)] = ... \n",
    "                        gradientes[\"b\" + str(l)] = ...\n",
    "    \n",
    "    Salida:\n",
    "        parametros: diccionario de python que contiene los parámetros actualizados \n",
    "                    parametros[\"W\" + str(l)] = ... \n",
    "                    parametros[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parametros) // 2 # número de capas en la red neuronal\n",
    "    \n",
    "    # Se actualiza cada uno de los parámetros. En el caso de una red profunda de L capas\n",
    "    # se hace con un loop que va recorriendo cada parámetro\n",
    "    for l in range(1,L+1):\n",
    "        \n",
    "        ####################################################################################\n",
    "        ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "        ####################################################################################\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        ####################################################################################\n",
    "        ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "        ####################################################################################\n",
    "    \n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción de la calidad del vino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizarán los bloques implementados anteriormente para entrenar una red neuronal que estime la calidad de una variante de *vino tinto* en función de propiedades químicas de los mismos. Se cuenta con un conjunto de datos etiquetados por expertos con valores que se encuentran en el rango 0 (pésima calidad) a 10 (excelente calidad). La calificación asociada a cada muestra corresponde al promedio de calificaciones de tres expertos. Los datos fueron obtenidos de la plataforma [Kaggle](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = ['fixed acidity','volatile acidity','citric acid','residual sugar','chlorides',\n",
    "                  'free sulfur dioxide','total sulfur dioxide','density','pH','sulphates','alcohol','quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('wine/winequality-red.csv', skiprows = 1, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda muestra los histogramas de las características y de la calidad (última gráfica). Observar que el rango efectivo de calidades es [3, 8]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "for i in range(12):\n",
    "    plt.subplot(3,4,i+1)\n",
    "    plt.hist(data[:,i])\n",
    "    plt.title(features_names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se separan los datos disponibles en dos conjuntos, uno de entrenamiento con dos tercios de los datos datos disponibles y otro de validación con el tercio restante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:,:11]\n",
    "y = data[:,11]\n",
    "N = len(X)\n",
    "indices = np.random.permutation(N)\n",
    "indices_train = indices[:2*N//3]\n",
    "indices_val = indices[2*N//3:]\n",
    "X_train = X[indices_train]\n",
    "X_val = X[indices_val]\n",
    "y_train = y[indices_train, None]\n",
    "y_val = y[indices_val, None]\n",
    "print('El conjunto de entrenamiento tiene %d muestras y el de validación %d muestras'  % (len(X_train), len(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte g) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplicará como técnica de preprocesamiento la *estandarización* de los datos. Para ello completar: i) la implementación del método `estandarizar_caracteristicas()` y ii) el código de la celda que realiza la estandarización. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estandarizar_caracteristicas(X, mu_sigma = None):\n",
    "    '''\n",
    "    Entrada:\n",
    "        X: matriz de tamaño Nxd que en cada fila contiene un vector de características\n",
    "        mu_sigma: matríz de tamaño 2xd que contiene los valores mu y sigma que se utilizan para \n",
    "                  hacer la transformación de las características. Cuando es igual a None, \n",
    "                  los valores de mu_sigma se calculan utilizando los datos de entrada X. La primera\n",
    "                  fila de la matriz contiene el valor medio de cada una de las características y la\n",
    "                  segunda la desviación estándar.\n",
    "    Salida:\n",
    "        Xstd: matriz de tamaño Nxd que en cada fila contiene un vector de \n",
    "              características con sus características estandarizadas\n",
    "        mu_sigma: matriz de tamaño 2xd que contiene los valores mu y sigma que se utilizaron para \n",
    "                  hacer la transformación de las características.\n",
    "    '''\n",
    "    \n",
    "    ##########################################################################################\n",
    "    ################ EMPIEZA ESPACIO PARA COMPLETAR CÓDIGO  ##################################\n",
    "    ##########################################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    ##########################################################################################\n",
    "    ################ TERMINA ESPACIO PARA COMPLETAR CÓDIGO  ##################################\n",
    "    ##########################################################################################\n",
    "    \n",
    "    return Xstd, mu_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validar_resultado('estandarizar_caracteristicas', funcion=estandarizar_caracteristicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ##########################################################################################\n",
    " ################ EMPIEZA ESPACIO PARA COMPLETAR CÓDIGO  ##################################\n",
    " ##########################################################################################\n",
    "\n",
    "# Se estandarizan los datos de entrenamiento y validación\n",
    "# X_train_std = \n",
    "# X_val_std = \n",
    "\n",
    "##########################################################################################\n",
    "################ TERMINA ESPACIO PARA COMPLETAR CÓDIGO  ##################################\n",
    "##########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red para predecir la calidad del vino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para predecir la calidad de los vinos se utilizará una red de **tres capas** con la siguiente arquitectura:   \n",
    "\n",
    "*Afin->Relu->Afin->Relu-->Afin-->Relu* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte i) \n",
    "\n",
    "Completar la implementación del método `predecir_calidad()`. Dicho método estima la calidad de *N* muestras a partir de sus propiedades químicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_calidad(X, parametros):\n",
    "    \"\"\"\n",
    "    Esta función predice la calida de los vinos utilizando los pesos pasados como argumento. \n",
    "    La arquitectura de la red es la siguiente:\n",
    "    \n",
    "    Afin->Relu->Afin->Relu-->Afin-->Relu\n",
    "    \n",
    "    Entrada:\n",
    "        X: matriz de tamaño Nx11 que en cada fila contiene un vector de características\n",
    "        parametros: parametros de la red\n",
    "    \n",
    "    Salida:\n",
    "        p : vector de tamaño Nx1 que contiene las calidades estimadas\n",
    "    \"\"\"\n",
    "    \n",
    "    # Se obtienen W1, b1, W2, b2, W3 y b3 del diccionario de parámetros.\n",
    "    W1 = parametros[\"W1\"]\n",
    "    b1 = parametros[\"b1\"]\n",
    "    W2 = parametros[\"W2\"]\n",
    "    b2 = parametros[\"b2\"]\n",
    "    W3 = parametros[\"W3\"]\n",
    "    b3 = parametros[\"b3\"]\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    p = np.zeros((N,1))  # se inicializan las predicciones\n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "    \n",
    "    # Se hace la propagación hacia adelante de los datos de entrada X. Tener en cuenta que la\n",
    "    # arquitectura utilizada en la red fue Afin-->Relu-->Afin-->Relu-->Afin-->Relu\n",
    "    # ~ 3 lineas de codigo\n",
    "\n",
    "    \n",
    "    \n",
    "    # Se obtienen las predicciones. \n",
    "    # ~ 1 linea de codigo\n",
    "    \n",
    "    \n",
    "    ####################################################################################\n",
    "    ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "    ####################################################################################\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validar_resultado('predecir_calidad', funcion=predecir_calidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte j)  \n",
    "Completar la implementación del método `fit()` utilizando los métodos *forward* y *backward* adecuados para la arquitectura utilizada. Como función de costo se utilizará *mse()*. La optimización de los parámetros se realizará utilizando el método *batch gradient descent*. Dicho método procesa las muestras en lotes de tamaño *batch_size*. Para simplificar la implementación, cuando el número de muestras no sea múltiplo del *batch_size*, se descartarán las muestras sobrantes de la división entera. Para evitar que siempre se descarten las mismas muestras, en cada época se modifica el orden en que se pasan las muestras por la red.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train, Y_train, parametros, num_epocas = 1000, batch_size=8, learning_rate = 1,\n",
    "        X_val=None, Y_val=None, semilla=100, ax=None):\n",
    "    \n",
    "    '''\n",
    "    Método que entrena una red neuronal de tres capas con arquitectura: Afin->Relu->Afin->Relu->Afin->Relu.\n",
    "    Se utiliza como función de costo el error cuadrático medio.\n",
    "    \n",
    "    Entrada:\n",
    "        X_train: muestras de entrenamiento de tamaño (N, d_0)\n",
    "        Y_train: etiquetas asociadas a las muestras de entrenamiento. De tamaño (N,1).\n",
    "        parametros: pesos iniciales de la red\n",
    "        num_epocas: número de épocas que se entrenará la red\n",
    "        batch_size: tamaño de batch\n",
    "        learning_rate: learning rate utilizado para la actualización mediante descenso por gradiente\n",
    "        X_val: muestras que se utilizarán para monitorear la evolución del entrenamiento. De tamaño (N_val, d0).\n",
    "        Y_val: etiquetas asociadas a las muestras de validación. De tamaño (N_val,1).\n",
    "        semilla: semilla utilizada para la generación de números aleatorios\n",
    "        ax: axis utilizado para mostrar el gráfico de evolución de entrenamiento\n",
    "        \n",
    "    Salida:\n",
    "        parametros: diccionario de python que contiene los pesos de la red encontrados durante el entrenamiento\n",
    "        costos_train: evolución de la función de costo con el conjunto de entrenamiento\n",
    "        costos_val:evolución de la función de costo con el conjunto de validación\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(semilla)\n",
    "    gradientes = {} # se inicializa el diccionario que almacena los gradientes\n",
    "    costos_train, costos_val = [], []     # lista que almacena los costos promedios\n",
    "    N = X_train.shape[0]  # número de muestras\n",
    "    \n",
    "    # Se obtienen los parámetros iniciales del diccionario de parámetros.\n",
    "    W1 = parametros[\"W1\"]\n",
    "    b1 = parametros[\"b1\"]\n",
    "    W2 = parametros[\"W2\"]\n",
    "    b2 = parametros[\"b2\"]\n",
    "    W3 = parametros[\"W3\"]\n",
    "    b3 = parametros[\"b3\"]\n",
    "    \n",
    "    num_batchs_por_epoca = N // batch_size\n",
    "\n",
    "    # Loop (descenso por gradiente)\n",
    "    for i in range(0, num_epocas):\n",
    "        \n",
    "        # Se inicializa el costo de entrenamiento promedio para la época \n",
    "        costo_entrenamiento_promedio_epoca = 0; \n",
    "        \n",
    "        # Se sortea el orden en que se pasarán las muestas por la red\n",
    "        indices = np.random.permutation(N)\n",
    "        \n",
    "        for j in range(num_batchs_por_epoca):\n",
    "        \n",
    "            ####################################################################################\n",
    "            ###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "            ####################################################################################\n",
    "            \n",
    "            # se obtienen las muestas que se utilizarán en esta iteración\n",
    "            # Xb = \n",
    "\n",
    "            \n",
    "            # Propagación hacia adelante: Afin -> Relu -> Afin -> Relu -> Afin -> Relu. \n",
    "            # Entradas: \"Xb, W1, b1\". Salidas: \"X1, cache1, X2, cache2, X3, cache3\".\n",
    "            \n",
    "\n",
    "\n",
    "            # Se calcula el costo para esta iteración y se inicia la propagación hacia atrás\n",
    "            \n",
    "             \n",
    "            # Se actualiza el costo promedio para la época. Se acumulan los costos por batch y se\n",
    "            # divide por el número de batchs por época\n",
    "            \n",
    "\n",
    "            # Propagación hacia atrás. \n",
    "            # Entradas: \"dX3, cache3, cache2, cache1\". \n",
    "            # Salidas: \"dX2, dW3, db3, dX1, dW2, db2, dW1, db1, dX0 (no utilizado)\".\n",
    "            \n",
    "            \n",
    "\n",
    "            # Se almacenan los gradientes recientemente calculados en el diccionario \n",
    "\n",
    "            \n",
    "            \n",
    "            # Se actualizan los parámetros\n",
    "            # parametros = \n",
    "\n",
    "            \n",
    "            \n",
    "            # Se obtienen los nuevos W1, b1, W2 y b2 del diccionario de parámetros.        \n",
    "\n",
    "            \n",
    "            \n",
    "            ####################################################################################\n",
    "            ###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "            ####################################################################################\n",
    "\n",
    "\n",
    "        # Se guarda el costo de entrenamiento\n",
    "        costos_train.append(costo_entrenamiento_promedio_epoca)  \n",
    "        \n",
    "        # Se guarda el costo de validación\n",
    "        y_pred = predecir_calidad(X_val, parametros)\n",
    "        costo_val, _ = mse(y_pred, y_val)\n",
    "        costos_val.append(costo_val)\n",
    "        \n",
    "        \n",
    "        if i%50 == 0:\n",
    "            mensaje = 'Epoca {}:'.format(i)\n",
    "            mensaje += ' costo entrenamiento = {:.03f}'.format(np.squeeze(costo_entrenamiento_promedio_epoca))\n",
    "            mensaje += ', costo validación = {:.03f}'.format(np.squeeze(costo_val))\n",
    "            \n",
    "            print(mensaje)\n",
    "        \n",
    "    return parametros, costos_train, costos_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_tres_capas(X_train, Y_train, dims_capas, num_epochs = 1000, batch_size=8, learning_rate = 1,\n",
    "                     X_val=None, Y_val=None, mostrar_costo=False, semilla=100, axis=None, ylim=None):\n",
    "    \"\"\"\n",
    "    Implementa una red neuronal de dos capas: Afin->Relu->Afin->Relu-->Afin->Relu.\n",
    "    \n",
    "    Entrada:\n",
    "        X_train: muestras de entrenamiento de tamaño (N, d_0)\n",
    "        Y_train: etiquetas asociadas a las muestras de entrenamiento. De tamaño (N,1).\n",
    "        dims_capas: dimensiones de las capas(d_0, d_1, d_2, d3)\n",
    "        num_iter: número de iteraciones del loop de optimización\n",
    "        num_epocas: número de épocas que se entrenará la red\n",
    "        batch_size: tamaño de batch\n",
    "        learning_rate: learning rate utilizado para la actualización mediante descenso por gradiente\n",
    "        X_val: muestras que se utilizarán para monitorear la evolución del entrenamiento. De tamaño (N_val, d0).\n",
    "        Y_val: etiquetas asociadas a las muestras de validación. De tamaño (N_val,1).\n",
    "        mostrar_costo: Si vale True, se muestra la evolución del costo.\n",
    "        semilla: semilla utilizada para la generación de números aleatorios\n",
    "        ax: axis utilizado para mostrar el gráfico de evolución de entrenamiento\n",
    "        ylim: rango de valores del eje y que se muestran en el gráfico de evolución del costo\n",
    "        \n",
    "    Salida:\n",
    "        parametros: un diccionario de python que contiene los parámetros aprendidos \n",
    "                    por la red: W1, W2, W3, b1, b2 y b3\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "        \n",
    "    # Se inicializan los parámetros del diccionario llamando a una de las \n",
    "    # funciones previamente implementadas\n",
    "    parametros_iniciales = inicializar_pesos(dims_capas, semilla=semilla)\n",
    "     \n",
    "    \n",
    "    parametros, costos_train, costos_val = fit(X_train, Y_train, parametros_iniciales, num_epochs, \n",
    "                                               batch_size, learning_rate, X_val, Y_val, semilla, axis)\n",
    "\n",
    "        \n",
    "    # se muestra el costo\n",
    "    if mostrar_costo:\n",
    "        if axis is None:\n",
    "            axis =  plt.figure(figsize=(10,7))\n",
    "        axis.plot(np.squeeze(costos_train), label='train')\n",
    "        axis.plot(np.squeeze(costos_val), label='val')\n",
    "            \n",
    "        axis.set_ylabel('Costo')\n",
    "        axis.set_xlabel('Época')\n",
    "        axis.set_title(\"Learning rate =\" + str(learning_rate) + ' , batch_size = ' + str(batch_size))\n",
    "        axis.legend()\n",
    "        if ylim is not None:\n",
    "            axis.set_ylim(ylim)\n",
    "        axis.grid()\n",
    "\n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define una arquitectura de red con 64 nodos en cada una de las capas ocultas y un nodo en la capa de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Se definen las constantes que determinan la arquitectura de la red ####\n",
    "d_0 = X_train.shape[1]   \n",
    "d_1 = 64\n",
    "d_2 = 64\n",
    "d_3 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda entrena la red durante **10 épocas** para distintos valores del *learning rate* y el tamaño de *batch*. Ejecutar la celda y observar las curvas de entrenamiento para cada una de las configuraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [1e-3,1e-2,1e-1]  # se definen las tasas de aprendizaje a probar\n",
    "batch_sizes = [32, 256, X_train.shape[0]]  # se definen los tamaños de batch a probar\n",
    "\n",
    "plt.figure(figsize = (15,15))\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    for j, bs in enumerate(batch_sizes):\n",
    "        ax = plt.subplot(3,3,3*i+j+1)\n",
    "        \n",
    "        # Se entrena la red durante 10 épocas con tasa de aprendizaje lr y tamaño de batch bs \n",
    "        parametros_red_3capas = red_tres_capas(X_train_std, y_train, dims_capas = [d_0, d_1, d_2, d_3], \n",
    "                                    learning_rate = lr, num_epochs = 10, batch_size=bs, \n",
    "                                    mostrar_costo=True, X_val = X_val_std, Y_val=y_val, axis=ax, ylim=(0,18))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte k)\n",
    "\n",
    "Asumiendo que al ejecutar la celda anterior obtuvo la gráfica que se muestra a continuación, comente cómo influyen el *learning rate* y el tamaño de *batch* en la optimización. En particular comente *pros* y *contras* de utilizar valores grandes y chicos de ambos hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/lr_batch.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Respuesta:**   \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda entrena durante 1000 épocas utilizando un *learning rate* de 0.01 y un tamaño de *batch*  igual a 32. La corrida puede demorar un par de minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7,7))\n",
    "parametros_red_3capas = red_tres_capas(X_train_std, y_train, dims_capas = [d_0, d_1, d_2, d_3], \n",
    "                                    learning_rate = 1e-2, num_epochs = 1000, batch_size=32, \n",
    "                                    mostrar_costo=True,  X_val = X_val_std, Y_val=y_val, \n",
    "                                     axis=plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente celda muestra cómo se ajustan los datos al conjunto de entrenamiento y al conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "ax = plt.subplot(1,2,1)\n",
    "mostrar_ajuste(y_train, predecir_calidad(X_train_std, parametros_red_3capas), ax, 'Conjunto de entrenamiento')\n",
    "ax = plt.subplot(1,2,2)\n",
    "mostrar_ajuste(y_val, predecir_calidad(X_val_std, parametros_red_3capas), ax, 'Conjunto de validación')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estime el promedio de las diferencias  (valores absolutos de las diferencias) entre los valores estimados y los reales para los conjuntos de entrenamiento y validación.\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{N}\\sum_{i=1}^N   |y_i - \\hat{y_i}|\n",
    "$$\n",
    "\n",
    "$N$ es el número de muestras del conjunto, $y_i$ es la calidad de la *i-ésima* muestra y $\\hat{y}_i$ es el valor estimado para la *i-ésima* muestra.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "###########  EMPIEZA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "####################################################################################\n",
    "\n",
    "# Se calcula el MAE entre las predicciones y los valores reales para el conjunto de entrenamiento\n",
    "# diff_train =\n",
    "\n",
    "print('El error en la predicción en entrenamiento es %.03f' % (diff_train.mean()))\n",
    "\n",
    "# Se calcula el MAE entre las predicciones y los valores reales para el conjunto de validación\n",
    "# diff_val =\n",
    "\n",
    "print('El error en la predicción en validación es %.03f' % (diff_val.mean()))\n",
    "\n",
    "####################################################################################\n",
    "###########  TERMINA ESPACIO PARA COMPLETAR CODIGO  ################################\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte m)\n",
    "Comente sobre el punto de funcionamiento de la red y proponga al menos **tres estrategias** que a su criterio podrían contribuir a obtener una red que produzca mejores resultados en producción que la entrenada en este ejercicio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
